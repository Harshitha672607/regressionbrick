name: 3 Load HR Compensation CSV Dataset
description: Downloads HR compensation CSV file and prepares train/test datasets for regression
inputs:
  - {name: cdn_url, type: String, description: 'CDN URL to download CSV file'}
  - {name: target_column, type: String, description: 'Target column name'}
  - {name: feature_columns, type: String, description: 'Comma-separated feature columns'}
  - {name: train_split, type: Float, description: 'Train split ratio'}
  - {name: shuffle_seed, type: Integer, description: 'Random seed for shuffling'}
outputs:
  - {name: train_data, type: Dataset}
  - {name: test_data, type: Dataset}
  - {name: dataset_info, type: DatasetInfo}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v19
    command:
      - sh
      - -c
      - |
        python - <<'EOF'
        import sys, os, pickle, json, io, pandas as pd, numpy as np
        import requests
        from urllib.parse import unquote
        from sklearn.model_selection import train_test_split

        # --- Get args ---
        cdn_url = sys.argv[1]
        target_column = sys.argv[2]
        feature_columns_str = sys.argv[3]
        train_split = float(sys.argv[4])
        shuffle_seed = int(sys.argv[5])
        train_data_path = sys.argv[6]
        test_data_path = sys.argv[7]
        dataset_info_path = sys.argv[8]

        print(f"HR Compensation CSV Loader")
        print(f"CDN URL: {cdn_url}")
        print(f"Target column: {target_column}")
        print(f"Feature columns: {feature_columns_str}")
        print(f"Train split: {train_split}")
        print(f"Shuffle seed: {shuffle_seed}")

        feature_columns = [col.strip() for col in feature_columns_str.split(',')]

        def download_and_process_hr_data(url, target_col, feature_cols):
            decoded_url = unquote(url)
            print(f"Downloading from decoded URL: {decoded_url}")
            response = requests.get(decoded_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=30)
            response.raise_for_status()
            df = pd.read_csv(io.StringIO(response.text))
            print(f"Loaded CSV with shape: {df.shape}")
            missing_cols = [col for col in [atarget_col] + feature_cols if col not in df.columns]
            if missing_cols:
                raise ValueError(f"Missing columns: {missing_cols}. Available: {list(df.columns)}")
            return df[feature_cols], df[target_col], df

        X, y, original_df = download_and_process_hr_data(cdn_url, target_column, feature_columns)

        if len(X) == 0:
            raise Exception("No data found in CSV")

        stratify_col = X["level"] if "level" in feature_columns else None
        if stratify_col is not None:
            print("Using stratification on 'level'")
        else:
            print("No 'level' column found for stratification")

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, 
            train_size=train_split, 
            random_state=shuffle_seed,
            shuffle=True,
            stratify=stratify_col
        )

        train_data = {
            "features": X_train,
            "targets": y_train,
            "feature_columns": feature_columns,
            "target_column": target_column
        }

        test_data = {
            "features": X_test,
            "targets": y_test,
            "feature_columns": feature_columns,
            "target_column": target_column
        }

        dataset_info = {
            "total_samples": len(X),
            "train_samples": len(X_train),
            "test_samples": len(X_test),
            "feature_columns": feature_columns,
            "target_column": target_column,
            "numeric_features": ["years_exp"],
            "categorical_features": ["level", "location", "function"],
            "target_stats": {
                "mean": float(np.mean(y)),
                "std": float(np.std(y)),
                "min": float(np.min(y)),
                "max": float(np.max(y)),
                "log_mean": float(np.mean(np.log(y))),
                "log_std": float(np.std(np.log(y)))
            },
            "train_split_ratio": train_split,
            "shuffle_seed": shuffle_seed,
            "data_shape": original_df.shape,
            "has_level_stratification": "level" in feature_columns
        }

        os.makedirs(os.path.dirname(train_data_path) or ".", exist_ok=True)
        with open(train_data_path, "wb") as f:
            pickle.dump(train_data, f)

        os.makedirs(os.path.dirname(test_data_path) or ".", exist_ok=True)
        with open(test_data_path, "wb") as f:
            pickle.dump(test_data, f)

        os.makedirs(os.path.dirname(dataset_info_path) or ".", exist_ok=True)
        with open(dataset_info_path, "wb") as f:
            pickle.dump(dataset_info, f)

        print(f"Completed loading HR Compensation dataset.")
        EOF
    args:
      - {inputValue: cdn_url}
      - {inputValue: target_column}
      - {inputValue: feature_columns}
      - {inputValue: train_split}
      - {inputValue: shuffle_seed}
      - {outputPath: train_data}
      - {outputPath: test_data}
      - {outputPath: dataset_info}
