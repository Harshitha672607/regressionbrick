name: 9 Preprocess HR Compensation Data
description: Preprocesses HR compensation data with OneHotEncoding and log transformation
inputs:
  - name: train_data
    type: Dataset
  - name: test_data
    type: Dataset
  - name: dataset_info
    type: DatasetInfo
  - name: model_config
    type: String
    description: 'Regression configuration as JSON string'
outputs:
  - name: processed_data_pickle
    type: Dataset
  - name: preprocessing_pipeline
    type: Model
  - name: weight_out
    type: String
    description: "Preprocessing config as JSON string"
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        python -c "
        import sys, os, pickle, json, pandas as pd, numpy as np
        from sklearn.compose import ColumnTransformer
        from sklearn.preprocessing import OneHotEncoder
        from sklearn.pipeline import Pipeline
        from sklearn.linear_model import LinearRegression
        from sklearn.compose import TransformedTargetRegressor
        
        print('Number of arguments received:', len(sys.argv))
        for i, arg in enumerate(sys.argv):
            if i == 4:  # config string is long
                print(f'  Argument {i}: {arg[:100]}...')
            else:
                print(f'  Argument {i}: {arg}')
        
        # Get args - FIXED: Expect 8 arguments total
        if len(sys.argv) < 8:
            raise ValueError(f'Expected 8 arguments, got {len(sys.argv)}')
        
        train_path = sys.argv[1]
        test_path = sys.argv[2]
        info_path = sys.argv[3]
        config_str = sys.argv[4]
        out_path = sys.argv[5]
        pipeline_path = sys.argv[6]
        weight_path = sys.argv[7]
        
        print('Starting HR compensation preprocessing...')
        print(f'Train path: {train_path}')
        print(f'Test path: {test_path}')
        
        # Simple wrapper class
        class DataWrapper:
            def __init__(self, data_dict): 
                self.__dict__.update(data_dict)
        
        # Load data
        train_df = pd.read_csv(train_path)
        test_df = pd.read_csv(test_path)
        with open(info_path, 'rb') as f: 
            dataset_info = pickle.load(f)
        
        print(f'Loaded train: {train_df.shape[0]} samples, test: {test_df.shape[0]} samples')
        print(f'Dataset info keys: {list(dataset_info.keys())}')
        
        # FIXED: Safe access to dataset_info with defaults
        target_col = dataset_info.get('target_column', 'salary')
        feature_columns = dataset_info.get('feature_columns', [col for col in train_df.columns if col != target_col])
        
        print(f'Target column: {target_col}')
        print(f'Feature columns: {feature_columns}')
        
        X_train = train_df.drop(columns=[target_col])
        y_train = train_df[target_col]
        X_test = test_df.drop(columns=[target_col])
        y_test = test_df[target_col]
        
        # FIXED: Define numeric and categorical features with safe defaults
        # Assuming common HR compensation dataset structure
        numeric_features = dataset_info.get('numeric_features', ['years_exp', 'age', 'performance_rating'])
        categorical_features = dataset_info.get('categorical_features', ['level', 'location', 'function', 'department', 'education'])
        
        # Filter to only include columns that actually exist in the data
        numeric_features = [feat for feat in numeric_features if feat in X_train.columns]
        categorical_features = [feat for feat in categorical_features if feat in X_train.columns]
        
        print(f'Numeric features: {numeric_features}')
        print(f'Categorical features: {categorical_features}')
        print(f'All columns in data: {list(X_train.columns)}')
        
        # Create preprocessing pipeline
        preprocess = ColumnTransformer(
            transformers=[
                ('num', 'passthrough', numeric_features),
                ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features),
            ]
        )
        
        # Create model pipeline with log transformation
        linreg = LinearRegression()
        model = TransformedTargetRegressor(
            regressor=Pipeline(steps=[('prep', preprocess), ('lin', linreg)]),
            func=np.log,
            inverse_func=np.exp,
            check_inverse=False
        )
        
        print('Preprocessing pipeline created successfully')
        
        # Create data wrapper
        data_wrapper = DataWrapper({
            'X_train': X_train,
            'y_train': y_train,
            'X_test': X_test,
            'y_test': y_test,
            'preprocessor': preprocess,
            'model_pipeline': model,
            'numeric_features': numeric_features,
            'categorical_features': categorical_features,
            'dataset_info': dataset_info
        })
        
        # Save processed data
        os.makedirs(os.path.dirname(out_path) or '.', exist_ok=True)
        with open(out_path, 'wb') as f: 
            pickle.dump(data_wrapper, f)
        
        # Save preprocessing pipeline
        os.makedirs(os.path.dirname(pipeline_path) or '.', exist_ok=True)
        with open(pipeline_path, 'wb') as f: 
            pickle.dump(preprocess, f)
        
        # Save weight config
        weight_config = {
            'preprocessing_complete': True,
            'numeric_features': numeric_features,
            'categorical_features': categorical_features,
            'feature_columns': feature_columns,
            'target_column': target_col,
            'model_type': 'linear_regression_log_target',
            'transformation': 'log_target',
            'train_samples': len(X_train),
            'test_samples': len(X_test)
        }
        
        os.makedirs(os.path.dirname(weight_path) or '.', exist_ok=True)
        with open(weight_path, 'w') as f: 
            json.dump(weight_config, f, indent=2)
        
        print('Preprocessing complete!')
        print(f'Output saved to: {out_path}')
        print(f'Pipeline saved to: {pipeline_path}')
        print(f'Config saved to: {weight_path}')
        print(f'Train features: {X_train.shape}, Test features: {X_test.shape}')
        " "$0" "$1" "$2" "$3" "$4" "$5" "$6"
    args:
      - {inputPath: train_data}
      - {inputPath: test_data}
      - {inputPath: dataset_info}
      - {inputValue: model_config}
      - {outputPath: processed_data_pickle}
      - {outputPath: preprocessing_pipeline}
      - {outputPath: weight_out}
