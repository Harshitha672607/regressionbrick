name: 3 Preprocess HR Compensation Data
description: Preprocesses HR compensation data with OneHotEncoding and log transformation
inputs:
  - name: train_data
    type: Dataset
  - name: test_data
    type: Dataset
  - name: dataset_info
    type: DatasetInfo
  - name: model_config
    type: String
    description: 'Regression configuration as JSON string'
outputs:
  - name: processed_data_pickle
    type: Dataset
  - name: preprocessing_pipeline
    type: Model
  - name: weight_out
    type: String
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        python - "$@" <<'PYCODE'
        import sys, os, json, pickle, pandas as pd, numpy as np
        from sklearn.compose import ColumnTransformer
        from sklearn.preprocessing import OneHotEncoder
        from sklearn.pipeline import Pipeline
        from sklearn.linear_model import LinearRegression
        from sklearn.compose import TransformedTargetRegressor

        print('Number of arguments received:', len(sys.argv))
        for i, arg in enumerate(sys.argv):
            print(f'  Argument {i}: {arg[:100] if len(str(arg)) > 100 else arg}')

        if len(sys.argv) < 8:
            raise ValueError(f'Expected 8 arguments, got {len(sys.argv)}')

        train_path = sys.argv[1]
        test_path = sys.argv[2]
        info_path = sys.argv[3]
        config_str = sys.argv[4]
        out_path = sys.argv[5]
        pipeline_path = sys.argv[6]
        weight_path = sys.argv[7]

        print('Starting HR compensation preprocessing...')

        train_df = pd.read_csv(train_path)
        test_df = pd.read_csv(test_path)
        with open(info_path, 'rb') as f:
            dataset_info = pickle.load(f)

        print(f'Loaded train: {train_df.shape[0]} samples, test: {test_df.shape[0]} samples')
        print(f'Feature columns: {dataset_info["feature_columns"]}')

        target_col = dataset_info['target_column']

        X_train = train_df.drop(columns=[target_col])
        y_train = train_df[target_col]
        X_test = test_df.drop(columns=[target_col])
        y_test = test_df[target_col]

        numeric_features = dataset_info.get('numeric_features', ['years_exp'])
        categorical_features = dataset_info.get('categorical_features', ['level', 'location', 'function'])

        print(f'Numeric features: {numeric_features}')
        print(f'Categorical features: {categorical_features}')

        preprocess = ColumnTransformer(
            transformers=[
                ('num', 'passthrough', numeric_features),
                ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features),
            ]
        )

        linreg = LinearRegression()
        model = TransformedTargetRegressor(
            regressor=Pipeline(steps=[('prep', preprocess), ('lin', linreg)]),
            func=np.log,
            inverse_func=np.exp,
            check_inverse=False
        )

        print('Preprocessing pipeline created successfully')

        class DataWrapper:
            def __init__(self, data_dict):
                self.__dict__.update(data_dict)

        data_wrapper = DataWrapper({
            'X_train': X_train,
            'y_train': y_train,
            'X_test': X_test,
            'y_test': y_test,
            'preprocessor': preprocess,
            'model_pipeline': model,
            'numeric_features': numeric_features,
            'categorical_features': categorical_features,
            'dataset_info': dataset_info
        })

        os.makedirs(os.path.dirname(out_path) or '.', exist_ok=True)
        with open(out_path, 'wb') as f:
            pickle.dump(data_wrapper, f)

        os.makedirs(os.path.dirname(pipeline_path) or '.', exist_ok=True)
        with open(pipeline_path, 'wb') as f:
            pickle.dump(preprocess, f)

        weight_config = {
            'preprocessing_complete': True,
            'numeric_features': numeric_features,
            'categorical_features': categorical_features,
            'feature_columns': dataset_info['feature_columns'],
            'target_column': dataset_info['target_column'],
            'model_type': 'linear_regression_log_target',
            'transformation': 'log_target_exp_inverse'
        }
        os.makedirs(os.path.dirname(weight_path) or '.', exist_ok=True)
        with open(weight_path, 'w') as f:
            json.dump(weight_config, f, indent=2)

        print('HR compensation preprocessing complete!')
        print(f'Output saved to: {out_path}')
        print(f'Pipeline saved to: {pipeline_path}')
        PYCODE
    args:
      - {inputPath: train_data}
      - {inputPath: test_data}
      - {inputPath: dataset_info}
      - {inputValue: model_config}
      - {outputPath: processed_data_pickle}
      - {outputPath: preprocessing_pipeline}
      - {outputPath: weight_out}
