name: 5 Linear Regression Evaluate Model
description: Evaluates trained HR Linear Regression model and generates comprehensive metrics
inputs:
  - name: trained_model
    type: Model
  - name: data_path
    type: Dataset
  - name: model_coefficients
    type: String
outputs:
  - name: metrics
    type: Metrics
  - name: metrics_json
    type: String
  - name: evaluation_report
    type: String
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        python -c "
        import sys, os, pickle, json, pandas as pd, numpy as np
        from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
        
        print('Number of arguments received:', len(sys.argv))
        for i, arg in enumerate(sys.argv):
            print(f'  Argument {i}: {arg}')
        
        # Get args - Expect 7 arguments total
        if len(sys.argv) < 7:
            raise ValueError(f'Expected 7 arguments, got {len(sys.argv)}')
        
        trained_model_path = sys.argv[1]
        data_path = sys.argv[2]
        model_coefficients_path = sys.argv[3]
        metrics_path = sys.argv[4]
        metrics_json_path = sys.argv[5]
        evaluation_report_path = sys.argv[6]
        
        print('Starting HR Linear Regression Evaluation')
        print(f'Trained model path: {trained_model_path}')
        print(f'Data path: {data_path}')
        print(f'Coefficients path: {model_coefficients_path}')
        
        # Define the DataWrapper class (MUST MATCH THE PREPROCESSING COMPONENT)
        class DataWrapper:
            def __init__(self, data_dict=None):
                if data_dict:
                    self.__dict__.update(data_dict)
        
        # Load trained model
        if not os.path.exists(trained_model_path):
            raise FileNotFoundError(f'trained_model does not exist: {trained_model_path}')
            
        if not os.path.exists(data_path):
            raise FileNotFoundError(f'data_path does not exist: {data_path}')
            
        try:
            with open(trained_model_path, 'rb') as f:
                model_pipeline = pickle.load(f)
            print('Trained model loaded successfully')
        except Exception as e:
            raise Exception(f'ERROR loading trained model: {e}')
            
        try:
            with open(data_path, 'rb') as f:
                data_wrapper = pickle.load(f)
            print('Data loaded successfully')
        except Exception as e:
            raise Exception(f'ERROR loading data: {e}')
            
        # Load coefficients
        try:
            with open(model_coefficients_path, 'r') as f:
                coefficients_csv = f.read()
            coefficients_df = pd.read_csv(pd.compat.StringIO(coefficients_csv))
            print('Coefficients loaded successfully')
        except Exception as e:
            print(f'Warning loading coefficients: {e}')
            coefficients_df = pd.DataFrame()
        
        # Extract test data - handle both DataWrapper and direct attribute access
        try:
            # If it's a DataWrapper object
            X_test = data_wrapper.X_test
            y_test = data_wrapper.y_test
            dataset_info = data_wrapper.dataset_info
            numeric_features = data_wrapper.numeric_features
            categorical_features = data_wrapper.categorical_features
        except AttributeError:
            # If it's a dictionary or different structure
            if hasattr(data_wrapper, '__dict__'):
                X_test = data_wrapper.__dict__.get('X_test')
                y_test = data_wrapper.__dict__.get('y_test')
                dataset_info = data_wrapper.__dict__.get('dataset_info', {})
                numeric_features = data_wrapper.__dict__.get('numeric_features', ['years_exp'])
                categorical_features = data_wrapper.__dict__.get('categorical_features', ['level', 'location', 'function'])
            else:
                # Assume it's a dictionary
                X_test = data_wrapper.get('X_test')
                y_test = data_wrapper.get('y_test')
                dataset_info = data_wrapper.get('dataset_info', {})
                numeric_features = data_wrapper.get('numeric_features', ['years_exp'])
                categorical_features = data_wrapper.get('categorical_features', ['level', 'location', 'function'])
        
        # Fallback for dataset_info
        if not dataset_info:
            dataset_info = {
                'feature_columns': list(X_test.columns) if X_test is not None else [],
                'target_column': 'salary'
            }
        
        print(f'Evaluating on {len(X_test)} test samples')
        print(f'Features: {dataset_info.get(\"feature_columns\", [])}')
        
        # Make predictions
        y_pred = model_pipeline.predict(X_test)
        
        # Calculate comprehensive metrics
        r2 = r2_score(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mse = mean_squared_error(y_test, y_pred)
        
        # Additional metrics
        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
        explained_variance = 1 - (np.var(y_test - y_pred) / np.var(y_test))
        
        # Percentage within different error ranges
        error_percentage_10 = np.mean(np.abs((y_test - y_pred) / y_test) <= 0.10) * 100
        error_percentage_20 = np.mean(np.abs((y_test - y_pred) / y_test) <= 0.20) * 100
        error_percentage_30 = np.mean(np.abs((y_test - y_pred) / y_test) <= 0.30) * 100
        
        # Create comprehensive metrics dictionary
        metrics = {
            'test_metrics': {
                'r2_score': float(r2),
                'mae': float(mae),
                'rmse': float(rmse),
                'mse': float(mse),
                'mape': float(mape),
                'explained_variance': float(explained_variance),
                'error_within_10pct': float(error_percentage_10),
                'error_within_20pct': float(error_percentage_20),
                'error_within_30pct': float(error_percentage_30)
            },
            'target_statistics': {
                'actual_mean': float(np.mean(y_test)),
                'actual_std': float(np.std(y_test)),
                'predicted_mean': float(np.mean(y_pred)),
                'predicted_std': float(np.std(y_pred)),
                'actual_min': float(np.min(y_test)),
                'actual_max': float(np.max(y_test)),
                'predicted_min': float(np.min(y_pred)),
                'predicted_max': float(np.max(y_pred))
            },
            'model_info': {
                'model_type': 'linear_regression_log_target',
                'total_samples': len(y_test),
                'features_used': dataset_info.get('feature_columns', []),
                'coefficients_count': len(coefficients_df) if not coefficients_df.empty else 0
            }
        }
        
        # Create simple evaluation report without complex formatting
        evaluation_report = f'HR Compensation Linear Regression Evaluation Report'
        evaluation_report += f'========='
        evaluation_report += f'Model Performance:'
        evaluation_report += f'RÂ² Score: {r2:.3f}'
        evaluation_report += f'Mean Absolute Error: {mae:,.0f}'
        evaluation_report += f'Root Mean Squared Error: {rmse:,.0f}'
        evaluation_report += f'Mean Absolute Percentage Error: {mape:.1f}%'
        evaluation_report += f'Prediction Accuracy:'
        evaluation_report += f'Predictions within 10% of actual: {error_percentage_10:.1f}%'
        evaluation_report += f'Predictions within 20% of actual: {error_percentage_20:.1f}%'
        evaluation_report += f'Predictions within 30% of actual: {error_percentage_30:.1f}%'
        evaluation_report += f'Target Statistics:\\n'
        evaluation_report += f'Actual Salary - Mean: {np.mean(y_test):,.0f}, Std: {np.std(y_test):,.0f}'
        evaluation_report += f'Predicted Salary - Mean: {np.mean(y_pred):,.0f}, Std: {np.std(y_pred):,.0f}'
        evaluation_report += f'Features Used: {\\", \\".join(dataset_info.get(\\"feature_columns\\", []))}'
        evaluation_report += f'Total Test Samples: {len(y_test)}'
        
        # Save outputs
        try:
            os.makedirs(os.path.dirname(metrics_path) or '.', exist_ok=True)
            os.makedirs(os.path.dirname(metrics_json_path) or '.', exist_ok=True)
            os.makedirs(os.path.dirname(evaluation_report_path) or '.', exist_ok=True)
            
            # Save metrics
            with open(metrics_path, 'w') as f:
                json.dump(metrics, f, indent=2)
            
            # Save metrics JSON
            with open(metrics_json_path, 'w') as f:
                json.dump(metrics, f, indent=2)
            
            # Save evaluation report
            with open(evaluation_report_path, 'w') as f:
                f.write(evaluation_report)
            
            print('Evaluation Complete')
            print('Test Performance')
            print(f'R^2:  {r2:.3f}')
            print(f'MAE:  {mae:,.0f}')
            print(f'RMSE: {rmse:,.0f}')
            print(f'MAPE: {mape:.1f}%')
            
        except Exception as e:
            raise Exception(f'ERROR saving results: {e}')
        
        print('Model evaluation completed successfully!')
        print(f'Metrics saved to: {metrics_path}')
        print(f'Evaluation report saved to: {evaluation_report_path}')
        " -- "$0" "$1" "$2" "$3" "$4" "$5" "$6"
    args:
      - {inputPath: trained_model}
      - {inputPath: data_path}
      - {inputPath: model_coefficients}
      - {outputPath: metrics}
      - {outputPath: metrics_json}
      - {outputPath: evaluation_report}
