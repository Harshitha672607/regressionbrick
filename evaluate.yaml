name: 1 Linear Regression Evaluate Model
description: Evaluates trained HR Linear Regression model and generates comprehensive metrics
inputs:
  - name: trained_model
    type: Model
  - name: data_path
    type: Dataset
  - name: model_coefficients
    type: String
outputs:
  - name: metrics
    type: Metrics
  - name: metrics_json
    type: String
  - name: evaluation_report
    type: String
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        python - "$@" <<'PYCODE'
        import sys, os, pickle, json, pandas as pd, numpy as np
        from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

        print("Number of arguments received:", len(sys.argv))
        for i, arg in enumerate(sys.argv):
            print(f"  Argument {i}: {arg}")

        # Expect: 1 script + 6 args = 7 total
        if len(sys.argv) != 7:
            raise ValueError(f"Expected 7 arguments, got {len(sys.argv)}")

        _, trained_model_path, data_path, model_coefficients_path, metrics_path, metrics_json_path, evaluation_report_path = sys.argv

        print("Starting HR Linear Regression Evaluation")
        print(f"Trained model path: {trained_model_path}")
        print(f"Data path: {data_path}")
        print(f"Coefficients path: {model_coefficients_path}")

        class DataWrapper:
            def __init__(self, data_dict=None):
                if data_dict:
                    self.__dict__.update(data_dict)

        # Load model
        with open(trained_model_path, 'rb') as f:
            model_pipeline = pickle.load(f)

        with open(data_path, 'rb') as f:
            data_wrapper = pickle.load(f)

        try:
            with open(model_coefficients_path, 'r') as f:
                coefficients_csv = f.read()
            coefficients_df = pd.read_csv(pd.compat.StringIO(coefficients_csv))
        except Exception:
            coefficients_df = pd.DataFrame()

        X_test = data_wrapper.X_test
        y_test = data_wrapper.y_test
        dataset_info = getattr(data_wrapper, 'dataset_info', {})
        if not dataset_info:
            dataset_info = {
                'feature_columns': list(X_test.columns),
                'target_column': 'salary'
            }

        print(f"Evaluating on {len(X_test)} samples")

        y_pred = model_pipeline.predict(X_test)

        # Metrics
        r2 = r2_score(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100

        metrics = {
            'r2_score': r2,
            'mae': mae,
            'rmse': rmse,
            'mape': mape
        }

        evaluation_report = f'''
        HR Linear Regression Evaluation Report
        =======================================
        RÂ² Score: {r2:.3f}
        MAE: {mae:,.2f}
        RMSE: {rmse:,.2f}
        MAPE: {mape:.2f}%
        '''

        os.makedirs(os.path.dirname(metrics_path) or '.', exist_ok=True)
        with open(metrics_path, 'w') as f:
            json.dump(metrics, f, indent=2)

        with open(metrics_json_path, 'w') as f:
            json.dump(metrics, f, indent=2)

        with open(evaluation_report_path, 'w') as f:
            f.write(evaluation_report)

        print("Evaluation complete!")
        print(evaluation_report)
        PYCODE
    args:
      - {inputPath: trained_model}
      - {inputPath: data_path}
      - {inputPath: model_coefficients}
      - {outputPath: metrics}
      - {outputPath: metrics_json}
      - {outputPath: evaluation_report}
